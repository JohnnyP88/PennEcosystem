{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install bs4\n",
    "# pip install selenium\n",
    "# pip install requests\n",
    "# pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/analytics-vidhya/web-scraping-google-search-results-with-selenium-and-beautifulsoup-4c534817ad88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "# Import the beautifulsoup and request libraries of python.\n",
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(), chrome_options=chrome_options)\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import Num\n",
    "from magic_google import MagicGoogle\n",
    "import pprint\n",
    "\n",
    "PROXIES = None\n",
    "# PROXIES = [{\n",
    "#     'http': 'http://192.168.2.207:1080',\n",
    "#     'https': 'http://192.168.2.207:1080'\n",
    "# }]\n",
    "\n",
    "# Or MagicGoogle()\n",
    "mg = MagicGoogle(PROXIES)\n",
    "links=[]\n",
    "# Crawling url\n",
    "for url in mg.search_url(query='site:upenn.edu \"innovation\"', num=1000):\n",
    "    links.append(url)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ECOSYSTEM_PENN_LIST = [\n",
    "'https://www.sas.upenn.edu/',\n",
    "'https://venturelab.upenn.edu/',\n",
    "'https://www.asc.upenn.edu/',\n",
    "'https://www.dental.upenn.edu/',\n",
    "'https://www.design.upenn.edu/',\n",
    "'https://www.gse.upenn.edu/',\n",
    "'https://www.seas.upenn.edu/',\n",
    "'https://www.law.upenn.edu/',\n",
    "'https://www.med.upenn.edu/',\n",
    "'https://www.nursing.upenn.edu/',\n",
    "'https://www.sp2.upenn.edu/',\n",
    "'https://www.vet.upenn.edu/',\n",
    "'https://pci.upenn.edu/',\n",
    "'https://www.library.upenn.edu/',\n",
    "'https://www.chop.edu/',\n",
    "'https://sciencecenter.org/',\n",
    "'https://wistar.org/',\n",
    "'https://www.upenn.edu/research-and-innovation',\n",
    "'https://research.upenn.edu/',\n",
    "'https://research.upenn.edu/centers-and-institutes/',\n",
    "'https://www.pennovation.upenn.edu/the-community#innovators',\n",
    "'https://penntoday.upenn.edu/subtopic/innovation',\n",
    "'https://yprize.upenn.edu/']\n",
    "df5 = pd.DataFrame(ECOSYSTEM_PENN_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('GoogleSearchData/siteupenn.edu innovation (1).csv',delimiter=';')\n",
    "df2 = pd.read_csv('GoogleSearchData/siteupenn.edu innovation (2).csv',delimiter=';')\n",
    "df3 = pd.read_csv('GoogleSearchData/siteupenn.edu innovation (3).csv',delimiter=';')\n",
    "df4 = pd.read_csv('GoogleSearchData/siteupenn.edu innovation (4).csv',delimiter=';')\n",
    "df1 = df1.Url\n",
    "df2 = df2.Url\n",
    "df3 = df3.Url\n",
    "df4 = df4.Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import replace\n",
    "df = pd.concat([df1,df2,df3,df4,df5])\n",
    "df = df.reset_index().drop(columns=['index'])\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.rename(columns={0:'url'}, inplace=True)\n",
    "df.to_csv('total_links.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "links = pd.read_csv('total_links.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(links.url.duplicated()) # check for duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(links[links.url.str.endswith(\"pdf\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude pdf links\n",
    "links = links[~links.url.str.endswith(\"pdf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = []\n",
    "for link in links.url:\n",
    "    link_element = link.split('/')\n",
    "    domains.append(\"https://\" + link_element[2].replace(\"www.\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links['domain'] = domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(domains)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://upenn.edu', verify=False)\n",
    "count = sum(page.text.lower().count(x) for x in ('innovation','innovative','innovat'))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "innovate_count = []\n",
    "for i in links.url:\n",
    "    page = requests.get(i, verify=False)\n",
    "    count = sum(page.text.lower().count(x) for x in ('innovation','innovative','innovat'))\n",
    "    innovate_count.append(count)\n",
    "innovate_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links['innovate_count'] = innovate_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links[links['innovate_count']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collapsed spreadsheet: domains, Col: how many pages are there, and how many times innovation is mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links.groupby('domain')['url'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_domain = pd.DataFrame(links.groupby('domain')['url'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_domain['innovate_count'] = links.groupby('domain')['innovate_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_domain.rename(columns={'url':'page_count'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_domain[groupby_domain['innovate_count']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "site-site matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('site_domainV2.xlsx') as writer:  \n",
    "    links.to_excel(writer, sheet_name='site', index=False)\n",
    "    groupby_domain.to_excel(writer, sheet_name='domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_dir = 'home/mgmt/wanxing/html_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "link = links.url[2]\n",
    "domain_name=[]\n",
    "print(link)\n",
    "link_element = link.split('/')\n",
    "link_element[2] = link_element[2].replace(\"www.\",\"\")\n",
    "reversed_element  = link_element[2].split('.')[::-1]\n",
    "print(reversed_element)\n",
    "reversed_element = ['.'.join(reversed_element[:i][::-1]) for i in range(1,len(reversed_element)+1)]\n",
    "print(reversed_element)\n",
    "link_element[2] = '/'.join(reversed_element)\n",
    "print(len(link_element))\n",
    "if len(link_element) <= 4:\n",
    "    domain_name.append(link_element[2])\n",
    "else:\n",
    "    for i in range(2, len(link_element)-2):\n",
    "        if len(link_element[i]) != 0:\n",
    "            domain_name.append(link_element[i])\n",
    "html_name = link_element[-2]\n",
    "# print(link_element)\n",
    "domain_name = '/'.join(domain_name)\n",
    "print(domain_name)\n",
    "print(link)\n",
    "filepath = '/Users/wanxing/PycharmProjects/PennEcosystem/html_data/c3i'\n",
    "filepath = os.path.join(destination_dir, domain_name)\n",
    "print(filepath)\n",
    "if not os.path.exists(filepath):\n",
    "    if not os.path.exists(destination_dir):\n",
    "        os.makedirs(destination_dir)\n",
    "    os.makedirs(filepath)\n",
    "filename = os.path.join(filepath, html_name+\".html\")\n",
    "urllib.request.urlretrieve(link, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lst= requests.get(url[0], verify=False)\n",
    "lst= sum(lst.text.count(x) for x in ('innovation', 'innovations', 'innovate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "destination_dir = 'home/mgmt/wanxing/html_data/'\n",
    "# print(link)\n",
    "for link in links:\n",
    "    domain_name=[]\n",
    "    link_element = link.split('/')\n",
    "    link_element[2] = link_element[2].replace(\"www.\",\"\").replace(\".edu\",\"\")\n",
    "    reversed_element  = link_element[2].split('.')[::-1]\n",
    "    link_element[2] = '/'.join(reversed_element)\n",
    "    for i in range(2, len(link_element)-2):\n",
    "        if len(link_element[i]) != 0:\n",
    "            domain_name.append(link_element[i])\n",
    "    name = link_element[-2]\n",
    "    # print(link_element)\n",
    "    domain_name = '/'.join(domain_name)\n",
    "    # print(domain)\n",
    "    # print(link)\n",
    "    # filepath = '/Users/wanxing/PycharmProjects/PennEcosystem/html_data/c3i'\n",
    "    filepath = os.path.join(destination_dir, domain_name)\n",
    "    # print(filepath)\n",
    "    if not os.path.exists(filepath):\n",
    "        if not os.path.exists(destination_dir):\n",
    "            os.makedirs(destination_dir)\n",
    "        os.makedirs(filepath)\n",
    "    filename = os.path.join(filepath, name+\".html\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(link, filename)\n",
    "    except:\n",
    "        continue\n",
    "    # print(filename)\n",
    "    print(\"Downloading \"+name)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Google search\n",
    "import time\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "text= \"innovation\"\n",
    "domain = \"q=site%3Aupenn.edu+\"\n",
    "links = [] # Initiate empty list to capture final results\n",
    "destination_dir = 'smb://hpcc.wharton.upenn.edu/wanxing/html_data/'\n",
    "# Specify number of pages on google search, each page contains 10 #links\n",
    "\n",
    "n_pages = 14200\n",
    "for page in range(1, n_pages):\n",
    "    # print(page)\n",
    "    url = \"https://www.google.com/search?\" + domain + text + \"&start=\" + str((page - 1) * 10)\n",
    "    # print(url)\n",
    "    driver.get(url)\n",
    "    soup = bs4.BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    # print(soup)\n",
    "    search = soup.find_all('div', class_=\"yuRUbf\")\n",
    "    # print(search)\n",
    "    for h in search:\n",
    "        links.append(h.a.get('href'))\n",
    "    for h in search:\n",
    "        domain=[]\n",
    "        link = h.a.get('href')\n",
    "        # print(link)\n",
    "        if link is not None:\n",
    "            link_element = link.split('/')\n",
    "            link_element[2] = link_element[2].replace(\"www.\",\"\").replace(\".upenn.edu\",\"\")\n",
    "            for i in range(2, len(link_element)-2):\n",
    "                if len(link_element[i]) != 0:\n",
    "                    domain.append(link_element[i])\n",
    "            name = link_element[-2]\n",
    "            # print(link_element)\n",
    "            domain = '/'.join(domain)\n",
    "            title = h.a.text\n",
    "            links.append(link)\n",
    "            # print(domain)\n",
    "            # print(link)\n",
    "            # filepath = '/Users/wanxing/PycharmProjects/PennEcosystem/html_data/c3i'\n",
    "            filepath = os.path.join(destination_dir, domain)\n",
    "            # print(filepath)\n",
    "            if not os.path.exists(filepath):\n",
    "                if not os.path.exists(destination_dir):\n",
    "                    os.makedirs(destination_dir)\n",
    "                os.makedirs(filepath)\n",
    "            filename = os.path.join(filepath, name+\".html\")\n",
    "            try:\n",
    "                urllib.request.urlretrieve(link, filename)\n",
    "            except:\n",
    "                continue\n",
    "            # print(filename)\n",
    "            print(\"Downloading \"+title)\n",
    "            time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "links = pd.DataFrame(links, columns=['link'])\n",
    "links.to_csv('links.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# import os\n",
    "# # Penn Website search https://www.upenn.edu/search\n",
    "# text= \"innovation\"\n",
    "# links = [] # Initiate empty list to capture final results\n",
    "# destination_dir = '/Users/wanxing/PycharmProjects/PennEcosystem/html_data/'\n",
    "# # Specify number of pages on google search, each page contains 10 #links\n",
    "\n",
    "# n_pages = 20\n",
    "# for page in range(1, n_pages):\n",
    "#    #https://www.upenn.edu/search?as_q=innovation#gsc.tab=0&gsc.q=innovation&gsc.page=1 \n",
    "#     url = \"https://www.upenn.edu/search?as_q=\" + text + \"#gsc.tab=0&gsc.q=innovation&gsc.page=\" + str((page))\n",
    "#     driver.get(url)\n",
    "#     soup = bs4.BeautifulSoup(driver.page_source, 'html.parser')\n",
    "#     search = soup.find_all('div', class_=\"gsc-thumbnail-inside\")\n",
    "#     for h in search:\n",
    "#         domain=[]\n",
    "#         link = h.a.get('href')\n",
    "#         # print(link)\n",
    "#         if link is not None:\n",
    "#             link_element = link.split('/')\n",
    "#             for i in range(2, len(link_element)):\n",
    "#                 if len(link_element[i]) != 0:\n",
    "#                     domain.append(link_element[i])\n",
    "#             domain = '/'.join(domain)\n",
    "#             title = h.a.text\n",
    "#             links.append(link)\n",
    "#             print(page)\n",
    "#             print(domain)\n",
    "#             print(link)\n",
    "\n",
    "#             filepath = os.path.join(destination_dir, domain)\n",
    "#             print(filepath)\n",
    "#             if not os.path.exists(filepath):\n",
    "#                 if not os.path.exists(destination_dir):\n",
    "#                     os.makedirs(destination_dir)\n",
    "#                 os.makedirs(filepath)\n",
    "#             try:\n",
    "#                 urllib.request.urlretrieve(link, filepath+'.html')\n",
    "#             except:\n",
    "#                 continue\n",
    "#             print(\"Downloading \"+title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   # Make two strings with default google search URL\n",
    "# # 'https://google.com/search?q=' and\n",
    "# # our customized search keyword.\n",
    "# # Concatenate them\n",
    "# text= \"innovation\"\n",
    "# domain = \"site%3Aupenn.edu+\"\n",
    "# url = 'https://google.com/search?q=' + domain + text\n",
    "  \n",
    "# # Fetch the URL data using requests.get(url),\n",
    "# # store it in a variable, request_result.\n",
    "# request_result=requests.get( url )\n",
    "  \n",
    "# # Creating soup from the fetched request\n",
    "# soup = bs4.BeautifulSoup(request_result.text,\n",
    "#                          \"html.parser\")\n",
    "# print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # soup.find.all( h3 ) to grab \n",
    "# # all major headings of our search result,\n",
    "# heading_object=soup.find_all( 'h3' )\n",
    "  \n",
    "# # Iterate through the object \n",
    "# # and print it as a string.\n",
    "# for info in heading_object:\n",
    "#     print(info.getText())\n",
    "#     print(\"------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
